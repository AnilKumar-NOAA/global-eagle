{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5a79557",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NOAA-EPIC/global-eagle/blob/feature/hello_world/examples/getting_started/colab_notebook_demo/pipeline_demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9aa7f4",
   "metadata": {},
   "source": [
    "# Welcome to the `ufs2arco` + `anemoi` + `wxvx` pipeline!\n",
    "\n",
    "Before we start, let's go over a few Google Colab tips!\n",
    "\n",
    "Q) Where are files located?!\n",
    "\n",
    "A) You should see a navigation bar on the left of your screen. The bottom option is a folder. Click on that and you will see all files in your workspace. If you have not run anything yet, you should only see a \"sample data\" folder (this automatically populates in any colab notebook). Throughout this notebook you can go into this area and watch your files populate, look at plots, and edit yamls if you wish to update any configurations on your own. Note: sometimes clicking through folders can be a little laggy.\n",
    "\n",
    "Q) How do I connect to compute?!\n",
    "\n",
    "A) You will need to connect to a runtime. Towards the top right of your screen you will see the words RAM and disk. There is a drop down button next to that. Click there, and then click \"change runtime type\". Make sure \"Python 3\" is selected under runtime type, and if available, select a T4 GPU as your hardware accelerator. If not available, you can run this notebook with a CPU but it will be very, very slow during training. If you happen to have credits for an A-100, use that!\n",
    "\n",
    "Now that you are connected to compute and know where to find your files, let's do some Machine Learning!\n",
    "\n",
    "This notebook will guide you through an entire ML pipeline.\n",
    "1) Data preprocessing using `ufs2arco` to create training and validation datasets\n",
    "2) Model training using `anemoi-core` modules to train a graph-based model\n",
    "3) Creating a forecast with `anemoi-inference` to run inference from a model checkpoint\n",
    "4) Verifying your forecast (or multiple!) with `wxvx` to verify against the Global Forecast System (GFS)\n",
    "\n",
    "More information about the various modules and instructions will be provided within each individual step. You will also find additional instructions if you wish to change configurations yourself at all!\n",
    "\n",
    "Acknowledgments:\n",
    "- ufs2arco and Anemoi configurations were adapted from Tim Smith at NOAA Physical Sciences Laboratory\n",
    "    - https://github.com/NOAA-PSL/anemoi-house\n",
    "- ufs2arco: Tim Smith (NOAA Physical Sciences Laboratory)\n",
    "    - https://github.com/NOAA-PSL/ufs2arco\n",
    "- Anemoi: European Centre for Medium-Range Weather Forecasts\n",
    "    - https://github.com/ecmwf/anemoi-core\n",
    "    - https://github.com/ecmwf/anemoi-inference\n",
    "- wxvx: Paul Madden (NOAA Global Systems Laboratory/Cooperative Institute for Research In Environmental Sciences)\n",
    "     - https://github.com/maddenp-cu/wxvx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0a0dbc",
   "metadata": {},
   "source": [
    "### Step 1: Environment Setup\n",
    "Runtime: 1 minute\n",
    "\n",
    "You will receive a popup after all packages are installed. Click \"restart session\" on the popup and continue on to the next step.\n",
    "\n",
    "You may see some red warnings about numpy versions. You can ignore this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5490612",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install anemoi-datasets==0.5.25 anemoi-graphs==0.6.2 anemoi-models==0.8.1 anemoi-training==0.5.1 anemoi-inference==0.6.3 trimesh 'numpy<2.3' 'earthkit-data<0.14.0' ufs2arco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7bbf06",
   "metadata": {},
   "source": [
    "Clone repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b04d60d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone -b feature/hello_world https://github.com/NOAA-EPIC/global-eagle.git\n",
    "\n",
    "#TODO -- right before merging to main we need to update this to not load branch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e75fb",
   "metadata": {},
   "source": [
    "### Step 2: Create training and validation datasets with ufs2arco\n",
    "\n",
    "Runtime: 3 minutes\n",
    "\n",
    "`ufs2arco` is a python package developed by NOAA Physical Sciences Laboratory (PSL) that is designed to make NOAA forecast, reanalysis, and reforecast datasets more accessible for scientific analysis and machine learning model development. The name stems from its original intent, which was to transform output from the Unified Forecast System (UFS) into Analysis Ready, Cloud Optimized (ARCO; Abernathey et al., (2021)) format. However, the package now pulls data from a number of non-UFS sources, including GFS/GEFS before UFS was created, and even ECMWF's ERA5 dataset.\n",
    "\n",
    "To learn more about ufs2arco, check out the documentation: https://ufs2arco.readthedocs.io/en/latest/index.html\n",
    "\n",
    "We are going to create the following dataset:\n",
    "- NOAA Replay Reanalysis\n",
    "- 3-hourly\n",
    "- Training data dates: 2022-01-01T00 - 2022-02-04T21\n",
    "- Validation data dates: 2022-01-03T00 - 2022-01-04T21\n",
    "- 1-degree global resolution\n",
    "\n",
    "For the purposes of running this notebook, we will not be creating a test set.\n",
    "\n",
    "While this cell is running, go into the `global-eagle/examples/getting_started/colab_notebook_demo/data` folder and look at `logs/logs.serial.out`. This will provide more insight into the dataset creation. Additionally, open `global-eagle/examples/getting_started/colab_notebook_demo/data/replay.yaml` to see all configurations related to data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b12fc4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!ufs2arco global-eagle/examples/getting_started/colab_notebook_demo/data/replay.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c91856",
   "metadata": {},
   "source": [
    "After the dataset has completed, let's view it!\n",
    "\n",
    "You will notice that this format looks different than a \"typical\" gridded netcdf or zarr file. The gridded data is flattened to be 1-dimensional, and we have calculated various statistics that will be used for normalization during training. These important details make the dataset ready to be used within a ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3045f3b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "ufs2arco_ds = xr.open_dataset(\"global-eagle/examples/getting_started/colab_notebook_demo/data/replay.zarr\")\n",
    "ufs2arco_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d7069d",
   "metadata": {},
   "source": [
    "### Step 3: Train a model with anemoi-core modules\n",
    "\n",
    "Runtime: 4 minutes\n",
    "\n",
    "We train a graph-based model with the `anemoi-core` modules from the European Centre for Medium-Range Weather Forecasts (ECMWF). The modules include the following:\n",
    "- `anemoi-graphs`: https://anemoi.readthedocs.io/projects/graphs/en/latest/\n",
    "- `anemoi-training`: https://anemoi.readthedocs.io/projects/training/en/latest/\n",
    "- `anemoi-models`: https://anemoi.readthedocs.io/projects/models/en/latest/\n",
    "\n",
    "Training is executed using `anemoi-training`.\n",
    "\n",
    "While training is running, go to the `global-eagle/examples/getting_started/colab_notebook_demo/train/training-output` folder. You will see folders containing checkpoints and plots from your run.\n",
    "\n",
    "We will use the following configurations to train the model:\n",
    "- Model task: Deterministic Forecasting (GraphForecaster)\n",
    "- Model type: Graph Transformer Neural Network\n",
    "- Graph: multi_scale encoder-processor-decoder configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd64dc25",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"ANEMOI_BASE_SEED\"] = \"42\"\n",
    "os.environ[\"SLURM_JOB_ID\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c94c685",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%cd global-eagle/examples/getting_started/colab_notebook_demo/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63903b02",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!anemoi-training train --config-name=config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fcff4e",
   "metadata": {},
   "source": [
    "### Step 4: Create a forecast with anemoi-inference\n",
    "\n",
    "Runtime: 12 seconds\n",
    "\n",
    "Documentation: https://anemoi.readthedocs.io/projects/inference/en/latest/\n",
    "\n",
    "Next, we will run inference using `anemoi-inference`. We will create a 48 hour forecast from 01/03/2022 0Z to 01/04/2022 21Z using a checkpoint from the model we just trained.\n",
    "\n",
    "Before executing the next two cells you will need to complete the following steps:\n",
    "1) Go to `global-eagle/examples/getting_started/colab_notebook_demo/train/training-output/checkpoints/` folder\n",
    "2) Copy the long id number found within that folder (e.g. `35a9632c-ab04-4284-af5e-4defcef37cff`)\n",
    "3) Open `global-eagle/examples/getting_started/colab_notebook_demo/inference/inference_config.yaml`\n",
    "4) Replace the checkpoint with the following: `\"../train/training-output/checkpoint/<ENTER YOUR ID HERE>/inference-last.ckpt\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc24e58",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%cd /content/global-eagle/examples/getting_started/colab_notebook_demo/inference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b8f922",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!anemoi-inference run inference_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b905dc4e",
   "metadata": {},
   "source": [
    "View inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a55f1f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "ds = xr.open_dataset(\"2022-01-03T00.nc\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa416743",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fhr = 1\n",
    "temp = ds['tmp2m'].isel(time=fhr).values\n",
    "lat = ds['latitude'].values\n",
    "lon = ds['longitude'].values\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(lon, lat, c=temp, s=10, cmap='coolwarm')\n",
    "plt.colorbar(label='2m Temperature')\n",
    "plt.title(f'2m Temperature at {ds[\"time\"][fhr].values}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c0e1c5",
   "metadata": {},
   "source": [
    "Postprocess inference\n",
    "\n",
    "We perform some postprocessing to ensure that the output will work with the wxvx framework for verification. This includes making the data 2D or 3D again, and adding necessary attributes required by wxvx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b0d045",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!python postprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22b2d08",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ds_post = xr.open_dataset(\"2022-01-03T00_postprocessed.nc\")\n",
    "ds_post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2180c7",
   "metadata": {},
   "source": [
    "Consider locally saving this final postprocessed netcdf file. This ensures that if you get disconnceted from this runtime, you can go run wxvx at a later time without having to rerun the whole notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61a1030",
   "metadata": {},
   "source": [
    "### Step 5: Verify the forecast against GFS with wxvx\n",
    "\n",
    "Runtime: 4 minutes\n",
    "\n",
    "`wxvx` is a workflow tool for verifying weather models. It leverages `uwtools` to drive `MET`. We are going to run grid-to-grid verification. Verification against observations is currently under development (coming soon!)\n",
    "\n",
    "First, Google Colab does not automatically come with Conda, so we have to install it. We will then run wxvx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be932852",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9890059c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!conda create -y -n wxvx -c ufs-community -c paul.madden wxvx python=3.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793aabc5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%cd global-eagle/examples/getting_started/colab_notebook_demo/verification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022b95a2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MPLBACKEND\"] = \"agg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653ddc60",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!conda run -n wxvx wxvx -c wxvx_config.yaml -t plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12a9d2",
   "metadata": {},
   "source": [
    "Now go to `global-eagle/examples/getting_started/colab_notebook_demo/verification/run/plots/20220103/00` and open some plots comparing (RMSE and ME) our model vs. GFS for numerous variables."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
